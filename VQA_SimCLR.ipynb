{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/bachn/miniconda3/envs/m2f/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-07 01:52:37.789590: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-07 01:52:37.789645: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-07 01:52:37.790894: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-07 01:52:37.799151: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-07 01:52:38.752087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from transformers import AutoTokenizer, RobertaModel\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train_data = []\n",
    "train_set_path = 'data/vaq2.0.TrainImages.txt'\n",
    "\n",
    "with open(train_set_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        temp = line.split('\\t')\n",
    "        qa = temp[1].split('?')\n",
    "\n",
    "        if len(qa) == 3:\n",
    "            answer = qa[2].strip()\n",
    "        else:\n",
    "            answer = qa[1].strip()\n",
    "\n",
    "        data_sample = {\n",
    "            'image_path': temp[0][:-2],\n",
    "            'question': qa[0] + '?',\n",
    "            'answer': answer\n",
    "        }\n",
    "        train_data.append(data_sample)\n",
    "\n",
    "# Load val data\n",
    "val_data = []\n",
    "val_set_path = 'data/vaq2.0.DevImages.txt'\n",
    "\n",
    "with open(val_set_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        temp = line.split('\\t')\n",
    "        qa = temp[1].split('?')\n",
    "\n",
    "        if len(qa) == 3:\n",
    "            answer = qa[2].strip()\n",
    "        else:\n",
    "            answer = qa[1].strip()\n",
    "\n",
    "        data_sample = {\n",
    "            'image_path': temp[0][:-2],\n",
    "            'question': qa[0] + '?',\n",
    "            'answer': answer\n",
    "        }\n",
    "        val_data.append(data_sample)\n",
    "\n",
    "# Load test data\n",
    "test_data = []\n",
    "test_set_path = 'data/vaq2.0.TestImages.txt'\n",
    "\n",
    "with open(test_set_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        temp = line.split('\\t')\n",
    "        qa = temp[1].split('?')\n",
    "\n",
    "        if len(qa) == 3:\n",
    "            answer = qa[2].strip()\n",
    "        else:\n",
    "            answer = qa[1].strip()\n",
    "\n",
    "        data_sample = {\n",
    "            'image_path': temp[0][:-2],\n",
    "            'question': qa[0] + '?',\n",
    "            'answer': answer\n",
    "        }\n",
    "        test_data.append(data_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = set([sample['answer'] for sample in train_data])\n",
    "\n",
    "classes_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "\n",
    "idx_to_classes = {idx: cls_name for idx, cls_name in enumerate(classes)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        classes_to_idx,\n",
    "        img_feature_extractor,\n",
    "        text_tokenizer,\n",
    "        device,\n",
    "        root_dir='/space/hotel/bachn/VQA/data/val2014-resised'\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.root_dir = root_dir\n",
    "        self.classes_to_idx = classes_to_idx\n",
    "        self.img_feature_extractor = img_feature_extractor\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.data[index]['image_path'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.img_feature_extractor:\n",
    "            img = self.img_feature_extractor(images=img, return_tensors=\"pt\")\n",
    "            img = {k: v.to(self.device).squeeze(0) for k, v in img.items()}\n",
    "\n",
    "        question = self.data[index]['question']\n",
    "        if self.text_tokenizer:\n",
    "            question = self.text_tokenizer(\n",
    "                question,\n",
    "                padding=\"max_length\",\n",
    "                max_length=20,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            question = {k: v.to(self.device).squeeze(0) for k, v in question.items()}\n",
    "\n",
    "        label = self.data[index]['answer']\n",
    "        label = torch.tensor(\n",
    "            classes_to_idx[label],\n",
    "            dtype=torch.long\n",
    "        ).to(self.device)\n",
    "\n",
    "        sample = {\n",
    "            'image': img,\n",
    "            'question': question,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_dataset = VQADataset(\n",
    "    train_data,\n",
    "    classes_to_idx=classes_to_idx,\n",
    "    img_feature_extractor=img_feature_extractor,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_dataset = VQADataset(\n",
    "    val_data,\n",
    "    classes_to_idx=classes_to_idx,\n",
    "    img_feature_extractor=img_feature_extractor,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_dataset = VQADataset(\n",
    "    test_data,\n",
    "    classes_to_idx=classes_to_idx,\n",
    "    img_feature_extractor=img_feature_extractor,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    device=device\n",
    ")\n",
    "train_batch_size = 128\n",
    "test_batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in train_loader:\n",
    "#     print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Loss Function (e.g., NT-Xent)\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature, device):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        N, Z = z_i.shape  # batch size and feature dimension\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "        sim = torch.mm(z, z.T) / self.temperature  # cosine similarity\n",
    "        sim_i_j = torch.diag(sim, N)\n",
    "        sim_j_i = torch.diag(sim, -N)\n",
    "        positives = torch.cat((sim_i_j, sim_j_i), dim=0).view(2 * N, 1)\n",
    "        negatives = sim[~torch.eye(2 * N, dtype=bool, device=self.device)].view(2 * N, -1)\n",
    "\n",
    "        labels = torch.zeros(2 * N).to(self.device).long()\n",
    "        logits = torch.cat((positives, negatives), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.pooler_output\n",
    "    \n",
    "    \n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VisualEncoder, self).__init__()\n",
    "        self.model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.pooler_output\n",
    "    \n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=768*2,\n",
    "        hidden_size=512,\n",
    "        n_layers=1,\n",
    "        dropout_prob=0.2,\n",
    "        n_classes=2\n",
    "    ):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        visual_encoder,\n",
    "        text_encoder,\n",
    "        classifier,\n",
    "        temperature=0.07  # Temperature parameter for contrastive loss\n",
    "    ):\n",
    "        super(VQAModel, self).__init__()\n",
    "        self.visual_encoder = visual_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.classifier = classifier\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, image, question, return_embedding=False):\n",
    "        text_out = self.text_encoder(question)\n",
    "        image_out = self.visual_encoder(image)\n",
    "        if return_embedding:\n",
    "            return text_out, image_out\n",
    "        combined = torch.cat((text_out, image_out), dim=1)\n",
    "        output = self.classifier(combined)\n",
    "        return output\n",
    "\n",
    "    def freeze(self, visual=True, textual=True, clas=False):\n",
    "        if visual:\n",
    "            for n, p in self.visual_encoder.named_parameters():\n",
    "                p.requires_grad = False\n",
    "        if textual:\n",
    "            for n, p in self.text_encoder.named_parameters():\n",
    "                p.requires_grad = False\n",
    "        if clas:\n",
    "            for n, p in self.classifier.named_parameters():\n",
    "                p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(classes)\n",
    "hidden_size = 1024\n",
    "n_layers = 1\n",
    "dropout_prob = 0.2\n",
    "\n",
    "text_encoder = TextEncoder().to(device)\n",
    "visual_encoder = VisualEncoder().to(device)\n",
    "classifier = Classifier(\n",
    "    hidden_size=hidden_size,\n",
    "    n_layers=n_layers,\n",
    "    dropout_prob=dropout_prob,\n",
    "    n_classes=n_classes\n",
    ").to(device)\n",
    "\n",
    "model = VQAModel(\n",
    "    visual_encoder=visual_encoder,\n",
    "    text_encoder=text_encoder,\n",
    "    classifier=classifier\n",
    ").to(device)\n",
    "model.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 149.0297, VQA Loss: 1.2482, Contrastive Loss: 147.7815\n",
      "Validation Loss: 0.6972, Validation Accuracy: 0.4621\n",
      "Epoch [2/50], Loss: 147.7884, VQA Loss: 0.7416, Contrastive Loss: 147.0468\n",
      "Validation Loss: 0.6983, Validation Accuracy: 0.5359\n",
      "Epoch [3/50], Loss: 147.8898, VQA Loss: 0.7478, Contrastive Loss: 147.1420\n",
      "Validation Loss: 0.7987, Validation Accuracy: 0.5359\n",
      "Epoch [4/50], Loss: 147.9100, VQA Loss: 0.7639, Contrastive Loss: 147.1460\n",
      "Validation Loss: 0.7976, Validation Accuracy: 0.4641\n",
      "Epoch [5/50], Loss: 147.7115, VQA Loss: 0.7486, Contrastive Loss: 146.9629\n",
      "Validation Loss: 0.8079, Validation Accuracy: 0.4641\n",
      "Epoch [6/50], Loss: 148.1384, VQA Loss: 0.7380, Contrastive Loss: 147.4004\n",
      "Validation Loss: 0.6898, Validation Accuracy: 0.5364\n",
      "Epoch [7/50], Loss: 148.0665, VQA Loss: 0.7341, Contrastive Loss: 147.3323\n",
      "Validation Loss: 0.7016, Validation Accuracy: 0.5369\n",
      "Epoch [8/50], Loss: 148.1217, VQA Loss: 0.7221, Contrastive Loss: 147.3997\n",
      "Validation Loss: 0.7355, Validation Accuracy: 0.4647\n",
      "Epoch [9/50], Loss: 148.1050, VQA Loss: 0.7468, Contrastive Loss: 147.3583\n",
      "Validation Loss: 0.6849, Validation Accuracy: 0.5671\n",
      "Epoch [10/50], Loss: 148.4845, VQA Loss: 0.7090, Contrastive Loss: 147.7754\n",
      "Validation Loss: 0.7342, Validation Accuracy: 0.5359\n",
      "Epoch [11/50], Loss: 147.9380, VQA Loss: 0.7415, Contrastive Loss: 147.1965\n",
      "Validation Loss: 0.6971, Validation Accuracy: 0.5466\n",
      "Epoch [12/50], Loss: 148.2761, VQA Loss: 0.7119, Contrastive Loss: 147.5642\n",
      "Validation Loss: 0.8170, Validation Accuracy: 0.4641\n",
      "Epoch [13/50], Loss: 149.1119, VQA Loss: 0.7241, Contrastive Loss: 148.3879\n",
      "Validation Loss: 0.7088, Validation Accuracy: 0.4959\n",
      "Epoch [14/50], Loss: 148.4066, VQA Loss: 0.7096, Contrastive Loss: 147.6970\n",
      "Validation Loss: 0.7942, Validation Accuracy: 0.5359\n",
      "Epoch [15/50], Loss: 147.9262, VQA Loss: 0.7292, Contrastive Loss: 147.1970\n",
      "Validation Loss: 0.6822, Validation Accuracy: 0.5615\n",
      "Epoch [16/50], Loss: 148.5359, VQA Loss: 0.7755, Contrastive Loss: 147.7604\n",
      "Validation Loss: 0.8801, Validation Accuracy: 0.4641\n",
      "Epoch [17/50], Loss: 148.5164, VQA Loss: 0.7327, Contrastive Loss: 147.7836\n",
      "Validation Loss: 0.8413, Validation Accuracy: 0.5364\n",
      "Epoch [18/50], Loss: 147.0425, VQA Loss: 0.7487, Contrastive Loss: 146.2938\n",
      "Validation Loss: 0.6789, Validation Accuracy: 0.5840\n",
      "Epoch [19/50], Loss: 148.8026, VQA Loss: 0.7292, Contrastive Loss: 148.0734\n",
      "Validation Loss: 1.0935, Validation Accuracy: 0.5359\n",
      "Epoch [20/50], Loss: 147.8284, VQA Loss: 0.7607, Contrastive Loss: 147.0677\n",
      "Validation Loss: 0.6803, Validation Accuracy: 0.5733\n",
      "Epoch [21/50], Loss: 147.7071, VQA Loss: 0.6954, Contrastive Loss: 147.0117\n",
      "Validation Loss: 0.9169, Validation Accuracy: 0.5359\n",
      "Epoch [22/50], Loss: 148.0614, VQA Loss: 0.7220, Contrastive Loss: 147.3394\n",
      "Validation Loss: 0.6967, Validation Accuracy: 0.5569\n",
      "Epoch [23/50], Loss: 147.2723, VQA Loss: 0.7181, Contrastive Loss: 146.5542\n",
      "Validation Loss: 0.6804, Validation Accuracy: 0.5978\n",
      "Epoch [24/50], Loss: 147.8769, VQA Loss: 0.7016, Contrastive Loss: 147.1753\n",
      "Validation Loss: 0.7016, Validation Accuracy: 0.5502\n",
      "Epoch [25/50], Loss: 148.1216, VQA Loss: 0.6918, Contrastive Loss: 147.4298\n",
      "Validation Loss: 0.7331, Validation Accuracy: 0.5092\n",
      "Epoch [26/50], Loss: 147.8281, VQA Loss: 0.7037, Contrastive Loss: 147.1244\n",
      "Validation Loss: 0.8076, Validation Accuracy: 0.4805\n",
      "Epoch [27/50], Loss: 148.0406, VQA Loss: 0.6852, Contrastive Loss: 147.3553\n",
      "Validation Loss: 0.7188, Validation Accuracy: 0.5400\n",
      "Epoch [28/50], Loss: 147.8006, VQA Loss: 0.7013, Contrastive Loss: 147.0993\n",
      "Validation Loss: 0.6817, Validation Accuracy: 0.5886\n",
      "Epoch [29/50], Loss: 148.4464, VQA Loss: 0.7241, Contrastive Loss: 147.7224\n",
      "Validation Loss: 0.7516, Validation Accuracy: 0.5282\n",
      "Epoch [30/50], Loss: 148.3183, VQA Loss: 0.6669, Contrastive Loss: 147.6514\n",
      "Validation Loss: 0.7082, Validation Accuracy: 0.5599\n",
      "Epoch [31/50], Loss: 148.0727, VQA Loss: 0.6462, Contrastive Loss: 147.4265\n",
      "Validation Loss: 0.6802, Validation Accuracy: 0.5891\n",
      "Epoch [32/50], Loss: 147.4967, VQA Loss: 0.6416, Contrastive Loss: 146.8551\n",
      "Validation Loss: 0.6821, Validation Accuracy: 0.5912\n",
      "Epoch [33/50], Loss: 146.8876, VQA Loss: 0.6343, Contrastive Loss: 146.2533\n",
      "Validation Loss: 0.6712, Validation Accuracy: 0.5948\n",
      "Epoch [34/50], Loss: 148.2256, VQA Loss: 0.6313, Contrastive Loss: 147.5942\n",
      "Validation Loss: 0.6747, Validation Accuracy: 0.5978\n",
      "Epoch [35/50], Loss: 147.1879, VQA Loss: 0.6260, Contrastive Loss: 146.5619\n",
      "Validation Loss: 0.6774, Validation Accuracy: 0.5968\n",
      "Epoch [36/50], Loss: 147.8408, VQA Loss: 0.6306, Contrastive Loss: 147.2102\n",
      "Validation Loss: 0.6678, Validation Accuracy: 0.5973\n",
      "Epoch [37/50], Loss: 147.7813, VQA Loss: 0.6217, Contrastive Loss: 147.1596\n",
      "Validation Loss: 0.6784, Validation Accuracy: 0.5927\n",
      "Epoch [38/50], Loss: 148.0600, VQA Loss: 0.6289, Contrastive Loss: 147.4311\n",
      "Validation Loss: 0.6676, Validation Accuracy: 0.6014\n",
      "Epoch [39/50], Loss: 148.2475, VQA Loss: 0.6208, Contrastive Loss: 147.6268\n",
      "Validation Loss: 0.6696, Validation Accuracy: 0.5999\n",
      "Epoch [40/50], Loss: 147.7484, VQA Loss: 0.6224, Contrastive Loss: 147.1260\n",
      "Validation Loss: 0.6662, Validation Accuracy: 0.5994\n",
      "Epoch [41/50], Loss: 148.6691, VQA Loss: 0.6192, Contrastive Loss: 148.0499\n",
      "Validation Loss: 0.6641, Validation Accuracy: 0.6030\n",
      "Epoch [42/50], Loss: 148.2648, VQA Loss: 0.6146, Contrastive Loss: 147.6502\n",
      "Validation Loss: 0.6632, Validation Accuracy: 0.6050\n",
      "Epoch [43/50], Loss: 148.2309, VQA Loss: 0.6093, Contrastive Loss: 147.6216\n",
      "Validation Loss: 0.6697, Validation Accuracy: 0.6101\n",
      "Epoch [44/50], Loss: 147.8590, VQA Loss: 0.6094, Contrastive Loss: 147.2496\n",
      "Validation Loss: 0.6612, Validation Accuracy: 0.6060\n",
      "Epoch [45/50], Loss: 147.7858, VQA Loss: 0.6001, Contrastive Loss: 147.1858\n",
      "Validation Loss: 0.6596, Validation Accuracy: 0.6142\n",
      "Epoch [46/50], Loss: 147.6014, VQA Loss: 0.6013, Contrastive Loss: 147.0001\n",
      "Validation Loss: 0.6639, Validation Accuracy: 0.6117\n",
      "Epoch [47/50], Loss: 148.0804, VQA Loss: 0.6040, Contrastive Loss: 147.4764\n",
      "Validation Loss: 0.6542, Validation Accuracy: 0.6224\n",
      "Epoch [48/50], Loss: 147.6500, VQA Loss: 0.5979, Contrastive Loss: 147.0521\n",
      "Validation Loss: 0.6602, Validation Accuracy: 0.6204\n",
      "Epoch [49/50], Loss: 147.9620, VQA Loss: 0.5942, Contrastive Loss: 147.3678\n",
      "Validation Loss: 0.6611, Validation Accuracy: 0.6255\n",
      "Epoch [50/50], Loss: 147.4123, VQA Loss: 0.5872, Contrastive Loss: 146.8251\n",
      "Validation Loss: 0.6534, Validation Accuracy: 0.6168\n",
      "Validation Loss: 0.6534193216777239, Validation Accuracy: 0.6168032786885246\n"
     ]
    }
   ],
   "source": [
    "# Training function with contrastive learning\n",
    "def fit_with_contrastive_learning(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, contrastive_loss_fn):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, total_contrastive_loss, total_vqa_loss = 0, 0, 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            images = batch['image']\n",
    "            questions = batch['question']\n",
    "            labels = batch['label']\n",
    "\n",
    "            # Forward pass for VQA task\n",
    "            outputs = model(images, questions)\n",
    "            vqa_loss = criterion(outputs, labels)\n",
    "\n",
    "            # Forward pass for contrastive learning\n",
    "            text_embedding, image_embedding = model(images, questions, return_embedding=True)\n",
    "            contrastive_loss = contrastive_loss_fn(text_embedding, image_embedding)\n",
    "\n",
    "            # Combine losses and backpropagate\n",
    "            loss = vqa_loss + contrastive_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_contrastive_loss += contrastive_loss.item()\n",
    "            total_vqa_loss += vqa_loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_contrastive_loss = total_contrastive_loss / len(train_loader)\n",
    "        avg_vqa_loss = total_vqa_loss / len(train_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, VQA Loss: {avg_vqa_loss:.4f}, Contrastive Loss: {avg_contrastive_loss:.4f}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch['image']\n",
    "            questions = batch['question']\n",
    "            labels = batch['label']\n",
    "\n",
    "            outputs = model(images, questions)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Instantiate the model, loss functions, optimizer, and scheduler\n",
    "contrastive_loss_fn = NTXentLoss(temperature=0.5, device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# Train the model\n",
    "fit_with_contrastive_learning(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50, contrastive_loss_fn=contrastive_loss_fn)\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
